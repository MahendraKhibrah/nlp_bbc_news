# -*- coding: utf-8 -*-
"""dicoding nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1evVPlLNzV4pYruO1LsdS-R_Er8dGYQ-o

nama : Mahendra krs

Proyek Pertama : Membuat Model NLP dengan TensorFlow
"""

import tensorflow as tf
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy()
print("Number of accelerators: ", strategy.num_replicas_in_sync)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv('drive/My Drive/bbc_data.csv')
df.info()

category = pd.get_dummies(df.labels)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='labels')

content = df_new['data'].values
label = df_new[['business', 'entertainment', 'politics', 'sport', 'tech']].values

from sklearn.model_selection import train_test_split
content_train, content_test, label_train, label_test = train_test_split(content, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(content_train)

sequence_train = tokenizer.texts_to_sequences(content_train)
sequence_test = tokenizer.texts_to_sequences(content_test)

padded_train = pad_sequences(sequence_train)
padded_test = pad_sequences(sequence_test)

max_length = 100

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, min_lr=0.0001)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])

num_epochs = 100
history = model.fit(padded_train, label_train, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2,
                    callbacks=[early_stopping, model_checkpoint, reduce_lr],
                    batch_size=64,
                    )

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')